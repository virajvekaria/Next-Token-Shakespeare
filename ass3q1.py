# -*- coding: utf-8 -*-
"""Ass3Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mupiItNoZV2lW1ATw0krz_rJ8NLsPa6F
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn.functional as F
from torch import nn
import pandas as pd
import matplotlib.pyplot as plt # for making figures
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'
from pprint import pprint
from sklearn.manifold import TSNE
import torch._dynamo
torch._dynamo.config.suppress_errors = True

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize empty lists to store dialogues
dialogues = []

# Read the text file line by line
with open('shakespeare.txt', 'r') as file:
    current_dialogue = ""
    for line in file:
        line = line.strip()  # Remove leading/trailing whitespace
        if line:  # Check if the line is not empty
            current_dialogue += line + " "
        else:
            # If line is empty, it indicates the end of a dialogue
            if current_dialogue:
                dialogues.append(current_dialogue.strip())  # Add the dialogue to the list
                current_dialogue = ""  # Reset current dialogue

# If there's a remaining dialogue after reading all lines
if current_dialogue:
    dialogues.append(current_dialogue.strip())

# Create a pandas DataFrame
df = pd.DataFrame({'Dialogue': dialogues})

words = df["Dialogue"]
words = words.str.lower()
words = words.str.strip()
words = words[2000]
# words = words.str.replace(" ", "")

# words = words[words.str.len() > 2]
# words = words[words.str.len() < 10]

# Randomly shuffle the words
words = words.sample(frac=1).reset_index(drop=True)
words = words.tolist()

# Remove words having non alphabets
# words = [word for word in words if word.isalpha()]

# build the vocabulary of characters and mappings to/from integers
chars = sorted(list(set(''.join(words))))
stoi = {s:i for i,s in enumerate(chars)}
# stoi['.'] = 0
itos = {i:s for s,i in stoi.items()}
pprint(itos)

block_size = 5 # context length: how many characters do we take to predict the next one?
X, Y = [], []
for w in words[:]:

  #print(w)
  context = [0] * block_size
  for ch in w + '.':
    ix = stoi[ch]
    X.append(context)
    Y.append(ix)
    print(''.join(itos[i] for i in context), '--->', itos[ix])
    context = context[1:] + [ix] # crop and append

# Move data to GPU

# print(len(X), len(Y))
X = torch.tensor(X).to(device)
Y = torch.tensor(Y).to(device)

# X.shape, X.dtype, Y.shape, Y.dtype

# Embedding layer for the context

emb_dim = 5
emb = torch.nn.Embedding(len(stoi), emb_dim)
emb.to(device)

# Function to visualize the embedding in 2d space

# def plot_emb(emb, itos, ax=None):
#     if ax is None:
#         fig, ax = plt.subplots()
#     for i in range(len(itos)):
#         # print(itos)
#         x, y = emb.weight[i].detach().cpu().numpy()
#         ax.scatter(x, y, color='k')
#         ax.text(x + 0.05, y + 0.05, itos[i])
#     return ax

# plot_emb(emb, itos)

# Function to visualize the embedding in 2d space

def plot_emb(emb, itos, ax=None, emb_dimensions=None):
    if ax is None:
        fig, ax = plt.subplots()

    if emb_dimensions is None:
        emb_dimensions = emb.weight.shape[1]

    if emb_dimensions == 2:
        # Directly plot if the embeddings are 2-dimensional
        for i in range(len(itos)):
            x, y = emb.weight[i].detach().cpu().numpy()
            ax.scatter(x, y, color='k')
            ax.text(x + 0.05, y + 0.05, itos[i])
    else:
        # Use t-SNE to reduce dimensionality for visualization
        tsne = TSNE(n_components=2, random_state=42)
        reduced_emb = tsne.fit_transform(emb.weight.detach().cpu().numpy())
        for i in range(len(itos)):
            x, y = reduced_emb[i]
            ax.scatter(x, y, color='k')
            ax.text(x + 0.05, y + 0.05, itos[i])

    return ax

# plot_emb(emb, itos)

class NextChar(nn.Module):
  def __init__(self, block_size, vocab_size, emb_dim, hidden_size):
    super().__init__()
    self.emb = nn.Embedding(vocab_size, emb_dim)
    self.lin1 = nn.Linear(block_size * emb_dim, hidden_size)
    self.lin2 = nn.Linear(hidden_size, hidden_size // 2)
    self.lin3 = nn.Linear(hidden_size // 2, vocab_size)


  def forward(self, x):
    x = self.emb(x)
    x = x.view(x.shape[0], -1)
    x = torch.sin(self.lin1(x))
    x = torch.relu(self.lin2(x))
    x = self.lin3(x)

    return x

# Generate names from untrained model
model = NextChar(block_size, len(stoi), emb_dim, 100).to(device)
model = torch.compile(model)

g = torch.Generator()
g.manual_seed(4000002)
def generate_name(model, itos, stoi, block_size, max_len=10):
    context = [0] * block_size
    name = ''
    for i in range(max_len):
        x = torch.tensor(context).view(1, -1).to(device)
        y_pred = model(x)
        ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item()
        ch = itos[ix]
        if ch == '.':
            break
        name += ch
        context = context[1:] + [ix]
    return name

for i in range(10):
    print(generate_name(model, itos, stoi, block_size))

for param_name, param in model.named_parameters():
    print(param_name, param.shape)

# Train the model

loss_fn = nn.CrossEntropyLoss()
opt = torch.optim.AdamW(model.parameters(), lr=0.01)
import time
# Mini-batch training
batch_size = 1000
print_every = 10
elapsed_time = []
for epoch in range(100):
    start_time = time.time()
    for i in range(0, X.shape[0], batch_size):
        x = X[i:i+batch_size]
        y = Y[i:i+batch_size]
        y_pred = model(x)
        loss = loss_fn(y_pred, y)
        loss.backward()
        opt.step()
        opt.zero_grad()
    end_time = time.time()
    elapsed_time.append(end_time - start_time)
    if epoch % print_every == 0:
        print(epoch, loss.item())

# Visualize the embedding

plot_emb(model.emb, itos)

# Generate names from trained model

def generate_tokens(model, itos, stoi, context, k):
    # Convert context string to list of indices
    context = [stoi[ch] for ch in context_str]

    generated_tokens = []
    for _ in range(k):
        x = torch.tensor(context).view(1, -1).to(device)
        y_pred = model(x)
        ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item()
        ch = itos[ix]
        generated_tokens.append(ch)
        context = context[1:] + [ix]  # Update context with the generated token
    return ''.join(generated_tokens)

# Example usage
context_str = 'thy n'  # Assuming block_size is 5
generated_string = generate_tokens(model, itos, stoi, context_str, k=100)
print('Generated string:', generated_string)

# Save the model
model_name = f'model_{emb_dim}_{block_size}_1.pth'
torch.save(model.state_dict(), model_name)
print(f'Model saved as {model_name}')


"""Tuning knobs

1. Embedding size
2. MLP
3. Context length
"""